##### hadoop优势
高可靠性：存储多个副本数据
高扩展性：集群间分发，可扩展上千个集群
高效性性：任务并行工作
高容错性：失败任务重新分发

##### hadoop组成
2.x:
    mapreduce:计算
    yarn:资源分配
    hdfs:数据存储
    (common)

#### HDFS架构
    namenode:存储文件的元数据，存储文件名，
    datanode：存储真是的数据，存储文件块数据，以及校验和
    2NN：辅助namenode:用来监视HDDDFS状态，每隔一段时间获取HDFS帮助namenode进行数据hebig
    
##### yarn架构
    rm:resourcemanager,处理客户端的请求，监控nodemanager，启动或监控appmaster, 资源的分配与调度
    nm:管理单个节点的资源，处理来自rm的命令，处理appmaster的命令
    appmaster：单个任务的老大，负责数据的切分，为应用程序申请资源，并分配内部的任务
    container:系统资源的抽象封装，他封装了某个节点上的多维度资源，如内存，磁盘网络等
    
    
#### HSFS缺点
    不适合低延时性数据访问
    无法高效对小文件进行存储
    不支持并发写入，文件随机修改
    
##### HDFS组成架构
    NameNode:集群的主管，管理者
        管理HDFS的名称空间
        配置副本策略
        管理数据块映射信息
        处理客户端读写请求
    DataNode:namenode下达命令，DataNode执行实际的操作
        存储实际的数据块
        执行数据块的读/写操作
    Client:客户端
        文件切分，文件上传HDFS时，将文件切分成一个一个Block，然后进行上传
        与那么node交互，获取文件的文职信息
        与datanode交互，读取或写入数据
        client提供一些命令管理HDFS，如namenode格式化
        client提供一些命令访问HDFS，如对HDFS的增删改查等操作
    Secondary NameNode:并非namenode的热备，当NameNode挂掉的时候，并不会马上替换
    namende并提供服务
        辅助namenode,分担工作量，如定期合并Fsimage和Edits,并推送给namenode
        紧急情况下，和辅助恢复namenode

#####HDFS写数据流程
    1.客户端向通过distributed FileSystem模块向Namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在
    2.namenode返回是否可以上传
    3.客户端请求上传第一个Block，
    namenode返回3个DataNode节点，分别是dn1,dn2,dn3
    Client通过fsDataOutput模块与三个datanode进行建立连接，依次建立连接，一次进行响应应答
    客户端向dn1,传输数据，dn1,传给dn2,dn2传给dn3
###### 副本节点选择
    第一个副本选择本节点上
    第二个副本放在同机架不同节点上
    第三个副本放在不同机架不同节点
##### hdfs读操作
    client向namenode申请下载资源，namenode查询后，返回元数据
    client挑选一台DataNode（就近原则）
    client客户端通过fsDataInputStream向datanode进行建立连接，建立连接后，进行传输数据

##### MapReduce概述
优点：
    易于编程
    良好扩展性
    高容错性
    适合PB级以上的海量数据的处理
缺点：
    不擅长实时计算
    不擅长流式计算
    不擅长DAG
##### MapReduce核心思想：
     MapReduce运算程序分为两个阶段Map阶段和Reduce阶段
     map阶段并发执行maptask任务
     reduce阶段进行汇总maptask的结果
#### MapReduce进程 
    MrAppMaster:负责整个程序的过程调度及状态协调
    Maptask：负责map阶段的整个数据处理流程
    reducetask：负责reduce阶段的这个数据处理流程    

#### Shuffle机制           
    待处理文本进行分割，切片
    将job.split jar程序和job.xml配置文件提交各 resourceManager
    resoucemanager根据任务计算出maptask的数量，启动对应数量的maptask
    mapTask默认调用TextInputFormat，调用RecordReader读取文本，以
    K,V值的形式， 然后调用mapper，进行相应的逻辑运算，map中调用
    Context.write（k,v）写个outputCollecotr ,将数据放在环形缓冲区，环形缓冲区默认100MB，分为两部分，左侧存储索引，元数据，右侧存储
    k,v键值对，当环形缓冲区数据达到80%后，将进行溢写，溢写过程中，先进行分区，排序，排序后溢写到文件（此时是一个分区且分区内有序的文件）
    多个分区且分区有序的文件，进行归并排序，归并排序从文件中读取数据，然后进行合并
    reduce阶段：
        reduce阶段从分别从不同的maptask中获取对应分区的数据，然后进行一次归并排序，将进行合并
        reducer一次读取一组，然后调用groupingComparaotr
        然后调用textoutputformat
        
##### mapReduce阶段优化
mapreduce程序效率的瓶颈在于两点：        
    1.计算机性能：
        CPU 内存 磁盘健康， 网络
    2.IO操作优化
        数据倾斜
        map和reduce数设置不合理
        map运行时间太长，导致reduce等待过久
        小文件过多
        大量的不可分块的超大文件
        splill次数过多（溢写次数过多）
        merge次数过多
MapReduce优化方法
    优化主要从6个方面考虑：数据输入，map阶段，reduce阶段， IO传输，数据倾斜问题和常用的调优参数
    数据输入：
        合并小文件
        采用CombineTextInputFOrmat作为输入，解决小文件场景
    map阶段
        减少溢写次数(spill)            
        减少合并次数(merge)
        在不影响业务逻辑的前提下，先进行Conbiner,减少IO
    reduce阶段
        合理设置map和reduce个数：
            map和reduce个数不能太多，也不能太少
         设置map，reduce共存：
         规避使用Reduce
         合理设置reduce端的buffer      
    IO传输：
        采用数据压缩的方式，减少IO
        使用sequenceFile二进制文件
    数据倾斜问题：
        抽样和范围分区：
            可以通过对原始数据进行抽样得到的结果集来预设分区边界值
        自定义分区：
        conmbine:   
        采用mapjoin,尽量避免Reduce join 
    常用参数调优：
           mapreduce.map.memory.mb
           mapreduce.reduce.memory.mb
           mapreduce.reduce.shuffle.paralleies:并行个数
             
           mapreduce.task.io.sort.mb         


####zookeeper

写数据流程：
    client向server1提交数据，发送一个请求
    如果server1不是leader，会把这个请求转交个leader,leader把这个消息群发给所有follower,比如server1,server3
    server1 和server3接收到请求后，将请求放入到消息队列中，然后向leader发送接收成功信息
    当leader接收到半数以上的成功信息，说明可以发送，就向各个server提交请求，然后server接收到后，会执行写请求，
    请求完成后，server1会向客户端发送成功的相应，结束
选举机制：
    以三台服务器为例：
    myid分别为1 2 3的机器
    1号先启动，发动一次投票，1号投自己一票，未达到半数，所以选举不成功，处于lokking状态
    2号启动，发送投票，2号投自己，1号发现2号myId比自己大，就把选票改为2，此时1号0票，2号2票，超过半数
    所以2号称为leader，1号称为follower
    3号启动时，发现1号 2号不是looking状态，已经有leader，3号投自己一票，3号一票，2号2票，少数服从多数，3号称为follower
    投票结束
paxos算法：
    是一种基于消息传递的具有高容错性的一致性算法

监听器原理：
    首先会有一个main方法，main方法中创建zookeeper客户端，然后创建两个线程
    一个connect(负责网络连接通信)，一个listenner(负责监听)
    通过connect 将注册的蒋婷是键发送给Zookeeper。
    在zookeeper的注册监听器列表中将注册的监听事件添加到列表中。
    如果被监听对象发生了改变，就通知listenner，listener条用process()方法，process方法可以自定义

节点类型：
    持久化
    非持久化
    有序节点
    非有序        
    
zookeeper常用命令：
ls path: 查看当前znode所包含的内容
ls2 path :查看当前znode做包含的信息，并显示更多的其他信息，如次更新等
create -s: 创建有序节点
        -e:创建临时节点
get path:获取节点的值
set: 设置节点的具体值
delete :删除节点
rmr: 递归删除节点。
    
部署方式；
    2种：本地单机模式， 集群模式
角色：
    leader foller
最少需要几台机器：
    最少需要三台，因为算法是基于paxos算法实现，基数台效果较好，    
        